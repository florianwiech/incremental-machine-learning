\chapter{Foundations}

\section{Tools}

\subsection*{Python}

Python is an interpreted, object-oriented, high-level programming language with dynamic semantics.
It is built for Rapid Application Development with and easy to learn syntax for readability.
Key features are built in data structures, dynamic typing and binding.
\cite{python-lang}
It has its own package manager called python Package Index (PyPi).
The language is compiled at execution and is available for all major platforms without charge.
The programming language is open source and maintained by the Pthon Software Foundation.
\cite{python-software-foundation}


\subsection*{NumPy}

NumPy is a package for Python.
It enables scientific computing with N-dimensional array objects and supports array broadcasting, type casting and basic linear algebra functions.
\cite{numpy-package}


\subsection*{Tensorflow}

Tensorflow is an entire ecosystem for solving problems with machine learning.
It is developed and maintained by the Google Brain team.
In 2015 it was released unser the Apache 2.0 open-source license.
It offers multiple levels of abstraction for building, training and testing models.
For more flexibility it allows visualisations of the code and intuitive debugging.
Large machine learning training tasks can be done with distributed training on different hardware configurations without changing the model definition.
Trained models can be directly put into production.
Moreover Tensorflow offers support for multiple languages and platforms.
\cite{tensorflow-about}

\subsubsection*{Core Concepts}

Tensorflow consists of a server client architecture.
The core is located on the server side and developed in C++.
Developers use client liberaries in different languages to interact with the core.
The general architecture for developers of Tensorflow is shown in figure \ref{fig:tensorflow_programming_environment_image}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{tensorflow_programming_environment}
    \caption{\cite{tensorflow_programming_environment_image} Tensorflow programming environment}
    \label{fig:tensorflow_programming_environment_image}
\end{figure}

The most important concept for developers is the dataflow graph functionality.
All variables, constants and operations form a computation graph.
Parts of the graph can be started and calculated across a set of local and remote devices.
Figure \ref{fig:tensorflow_graph_image} shows an example of a computation graph:

\begin{figure}[H]
    \centering
    \includegraphics[scale=.5]{tensors_graph}
    \caption{\cite{tensorflow_graph_image} Tensorflow graph}
    \label{fig:tensorflow_graph_image}
\end{figure}

\section{MNIST Dataset}

\begin{figure}[H]
    \centering
    \includegraphics{mnist_100_digits}
    \caption{\cite{mnist_examples_image} Examples of handwritten digits in the mnist dataset}
    \label{fig:mnist_examples}
\end{figure}

The MNIST database is a public available dataset of handwritten digits.
It contains a set of 70,000 entities split into 60,000 training and 10,000 test parts.
The digits are size-normalized and centered photos with a fixed-size of 28 by 28 pixels.
The numbers are created by approximately 250 writers. 
\cite{mnist-database}
The Dataset pertains to one of the most popular dataset for research and testing the performance of new networks.
It is a great resource for testing new deep learning algorithms with supervised learning.

\section{Deep Neural Network}

Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing.
\cite{nielsen-book}
They are loosly patterned after the brain structure.
The network uses connected layers of neurons and "teaches" itself how to understand data by classifying records or making predictions.
\cite{ibm-watson-healthcare}

…

Weights are the connections between neurons.
Each weights carries a value, which shows the importance of the neuron value from the inout side.
Each weights of a neuron shape a vector.
All weights vectors from all neurons on a layer create a matrix format.

Every neuron which is not on the input layer has a bias attached.
A bias carries like every weight a value.
All bias from neurons on one layer forge a vector.

Each neuron on the hidden layers feature a activation function.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        init/.style={
        draw,
        circle,
        inner sep=2pt,
        font=\Huge,
        join = by -latex
        },
        squa/.style={
        draw,
        inner sep=2pt,
        font=\Large,
        join = by -latex
        },
        start chain=2,node distance=13mm
        ]
        \node[on chain=2] 
        (x2) {$x_2$};
        \node[on chain=2,join=by o-latex] 
        {$w_2$};
        \node[on chain=2,init] (sigma) 
        {$\displaystyle\Sigma$};
        \node[on chain=2,squa,label=above:{\parbox{2cm}{\centering Activation \\ function}}]
        {$f$};
        \node[on chain=2,label=above:Output,join=by -latex] 
        {$y$};
        \begin{scope}[start chain=1]
            \node[on chain=1] at (0,1.5cm) 
            (x1) {$x_1$};
            \node[on chain=1,label=above:Weights,join=by o-latex] 
            (w1) {$w_1$};
        \end{scope}
        \begin{scope}[start chain=3]
            \node[on chain=3] at (0,-1.5cm) 
            (x3) {$x_3$};
            \node[on chain=3,join=by o-latex] 
            (w3) {$w_3$};
        \end{scope}
        \node[label=above:\parbox{2cm}{\centering Bias \\ $b$}] at (sigma|-w1) (b) {};
        
        \draw[-latex] (w1) -- (sigma);
        \draw[-latex] (w3) -- (sigma);
        \draw[o-latex] (b) -- (sigma);
        
        \draw[decorate,decoration={brace,mirror}] (x1.north west) -- node[left=10pt] {Inputs} (x3.south west);
    \end{tikzpicture}
    \caption{\cite{dnn_neuron_basic_overview} Neuron structure}
    \label{fig:dnn_node_procedure}
\end{figure}

Each node on the hidden layers have a weight and bias stored. The node gets a input vector from the layer before.

\subsection*{Mathematics}

A deep neural network for classifying the mnist dataset, it uses $28*28=784$ input parameters.
The output would be for each digit (0-9) a class, which results in 10 classes.
\section{Catastrophic Forgetting}

Catastrophic forgetting was first discovered in 1989 in the article "Catastrophic Intereference" by McCloskey and Cohen.
They broke the problem down into a rather simplified version to explain the problem:
Figure \ref{fig:catastrophic_forgetting_clarification}A shows a reprensentation of a network in a simplified form.
It has stored the facts $5 + 4 = 9$ and $6 + 4 = 10$.
\hfill \break
Now this network wants to learn new facts.
Figure \ref{fig:catastrophic_forgetting_clarification}B shows the appendix of the new fact $7 + 4 = 11$.
To be able to learn this fact it is represented seperate to the representation of other facts.
This storing of facts does not disrupt the representation of presious learned knowledge.
\hfill \break
But in a neural network the representation is quite different.
All weights are involved in responding to many different inputs.
To be able to predict the additional new desired outcome, it has to adjust the weights.
This process will alter the network's response to the other input patterns.
It results in changing weights to encode new information and alters previously learned responses to other inputs.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{McCloskeyCohen1989CatastropicInterference}
    \caption{\cite[page 148]{psychology_learning_mccloskey_cohen} Catastrophic forgetting clarification}
    \label{fig:catastrophic_forgetting_clarification}
\end{figure}


\section{Elastic Weight Consolidation}

…